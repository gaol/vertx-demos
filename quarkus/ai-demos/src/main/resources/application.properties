quarkus.http.port=8080

quarkus.langchain4j.image-model.provider=openai

# openai service
quarkus.langchain4j.openai.base-url=http://192.168.1.23:5000
quarkus.langchain4j.openai.image-model.response-format=b64_json
quarkus.langchain4j.openai.image-model.size=512x512
quarkus.langchain4j.openai.image-model.number=1
quarkus.langchain4j.openai.timeout=300s
quarkus.langchain4j.openai.log-requests=true
quarkus.langchain4j.openai.log-responses=false

quarkus.langchain4j.chat-model.provider=ollama

# ollama service
quarkus.langchain4j.ollama.base-url=http://192.168.1.53:11434
quarkus.langchain4j.ollama.chat-model.model-id=llama3
quarkus.langchain4j.ollama.chat-model.temperature=0.1
quarkus.langchain4j.ollama.timeout=300s
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true

# codellama
quarkus.langchain4j.codellama.chat-model.provider=ollama
quarkus.langchain4j.ollama.codellama.base-url=http://192.168.1.53:11434
quarkus.langchain4j.ollama.codellama.chat-model.model-id=codellama
quarkus.langchain4j.ollama.codellama.timeout=300s
quarkus.langchain4j.ollama.codellama.chat-model.num-predict=2048

# embedding-model will use mxbai-embed-large from ollama, which has dimension: 512
# the embedding store will use redis-stack docker image
quarkus.langchain4j.embedding-model.provider=ollama
quarkus.langchain4j.ollama.embedding-model.model-id=mxbai-embed-large
quarkus.langchain4j.redis.dimension=512
quarkus.redis.devservices.image-name=redis/redis-stack:7.2.0-v10-x86_64
quarkus.redis.hosts=redis://localhost:6379
